{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "hide_table_of_contents: true\n",
    "sidebar_position: 0\n",
    "sidebar_label: Tool calling\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool calling agent\n",
    "\n",
    "```{=mdx}\n",
    ":::info\n",
    "Tool calling is only available with [supported models](/docs/integrations/chat/).\n",
    ":::\n",
    "```\n",
    "\n",
    "[Tool calling](/docs/modules/model_io/chat/function_calling) allows a model to respond to a given prompt by generating output that matches a user-defined schema.\n",
    "By supplying the model with a schema that matches up with a [LangChain tool's](/docs/modules/agents/tools/) signature, along with a name and description of what the tool does, we can get the model\n",
    "to reliably generate valid input.\n",
    "\n",
    "We can take advantage of this structured output, combined with the fact that [tool calling chat models](/docs/integrations/chat/) can choose which tool to call in a given situation, to create an agent that repeatedly calls tools and receives results until a query is resolved.\n",
    "\n",
    "This is a more generalized version of the [OpenAI tools agent](/docs/modules/agents/agent_types/openai_tools_agent/), which was designed for OpenAI's specific style of\n",
    "tool calling. It uses LangChain's [ToolCall](https://api.js.langchain.com/types/langchain_core_messages_tool.ToolCall.html) interface to support a wider range of\n",
    "provider implementations, such as [Anthropic](/docs/integrations/chat/anthropic/), [Google Gemini](/docs/integrations/chat/google_vertex_ai), and [Mistral](/docs/integrations/chat/mistral/)\n",
    "in addition to [OpenAI](/docs/integrations/chat/openai/).\n",
    "\n",
    "## Setup\n",
    "\n",
    "Most models that support tool calling can be used in this agent. See [this list](/docs/integrations/chat/) for the most up-to-date information.\n",
    "\n",
    "This demo also uses [Tavily](https://app.tavily.com), but you can also swap in another [built in tool](/docs/integrations/platforms).\n",
    "You'll need to sign up for an API key and set it as `process.env.TAVILY_API_KEY`.\n",
    "\n",
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs customVarName=\"llm\" providers={[\"anthropic\", \"openai\", \"mistral\", \"fireworks\"]} additionalDependencies=\"@langchain/community\" />\n",
    "```\n",
    "\n",
    "## Initialize Tools\n",
    "\n",
    "We will first create a tool that can search the web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n",
    "\n",
    "// Define the tools the agent will have access to.\n",
    "const tools = [new TavilySearchResults({ maxResults: 1 })];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent\n",
    "\n",
    "Next, let's initialize our tool calling agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { createToolCallingAgent } from \"langchain/agents\";\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "\n",
    "// Prompt template must have \"input\" and \"agent_scratchpad input variables\"\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", \"You are a helpful assistant\"],\n",
    "  [\"placeholder\", \"{chat_history}\"],\n",
    "  [\"human\", \"{input}\"],\n",
    "  [\"placeholder\", \"{agent_scratchpad}\"],\n",
    "]);\n",
    "\n",
    "const agent = await createToolCallingAgent({\n",
    "  llm,\n",
    "  tools,\n",
    "  prompt,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Agent\n",
    "\n",
    "Now, let's initialize the executor that will run our agent and invoke it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  input: \"what is LangChain?\",\n",
      "  output: \"LangChain is an open-source framework for building applications with large language models (LLMs). S\"... 983 more characters\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { AgentExecutor } from \"langchain/agents\";\n",
    "\n",
    "const agentExecutor = new AgentExecutor({\n",
    "  agent,\n",
    "  tools,\n",
    "});\n",
    "\n",
    "const result = await agentExecutor.invoke({\n",
    "  input: \"what is LangChain?\",\n",
    "});\n",
    "\n",
    "console.log(result);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{=mdx}\n",
    ":::tip\n",
    "[LangSmith trace](https://smith.langchain.com/public/2f956a2e-0820-47c4-a798-c83f024e5ca1/r)\n",
    ":::\n",
    "```\n",
    "\n",
    "## Using with chat history\n",
    "\n",
    "This type of agent can optionally take chat messages representing previous conversation turns. It can use that previous history to respond conversationally. For more details, see [this section of the agent quickstart](/docs/modules/agents/quick_start#adding-in-memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  input: \"what's my name?\",\n",
      "  chat_history: [\n",
      "    HumanMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: {\n",
      "        content: \"hi! my name is cob\",\n",
      "        additional_kwargs: {},\n",
      "        response_metadata: {}\n",
      "      },\n",
      "      lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
      "      content: \"hi! my name is cob\",\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    AIMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: {\n",
      "        content: \"Hello Cob! How can I assist you today?\",\n",
      "        tool_calls: [],\n",
      "        invalid_tool_calls: [],\n",
      "        additional_kwargs: {},\n",
      "        response_metadata: {}\n",
      "      },\n",
      "      lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
      "      content: \"Hello Cob! How can I assist you today?\",\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {},\n",
      "      tool_calls: [],\n",
      "      invalid_tool_calls: []\n",
      "    }\n",
      "  ],\n",
      "  output: \"You said your name is Cob.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { AIMessage, HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const result2 = await agentExecutor.invoke({\n",
    "  input: \"what's my name?\",\n",
    "  chat_history: [\n",
    "    new HumanMessage(\"hi! my name is cob\"),\n",
    "    new AIMessage(\"Hello Cob! How can I assist you today?\"),\n",
    "  ],\n",
    "});\n",
    "\n",
    "console.log(result2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{=mdx}\n",
    ":::tip\n",
    "[LangSmith trace](https://smith.langchain.com/public/e21ececb-2e60-49e5-9f06-a91b0fb11fb8/r)\n",
    ":::\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
