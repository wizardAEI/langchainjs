---
sidebar_position: 5
---

# Tracking token usage

This notebook goes over how to track your token usage for specific calls.

## Using AIMessage.response_metadata

A number of model providers return token usage information as part of the chat generation response. When available, this is included in the [AIMessage.response_metadata](/docs/modules/model_io/chat/response_metadata/) field.
Here's an example with OpenAI:

import CodeBlock from "@theme/CodeBlock";
import Example from "@examples/models/chat/token_usage_tracking.ts";

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai
```

<CodeBlock language="typescript">{Example}</CodeBlock>

And here's an example with Anthropic:

import AnthropicExample from "@examples/models/chat/token_usage_tracking_anthropic.ts";

```bash npm2yarn
npm install @langchain/anthropic
```

<CodeBlock language="typescript">{AnthropicExample}</CodeBlock>

## Using callbacks

You can also use the `handleLLMEnd` callback to get the full output from the LLM, including token usage for supported models.
Here's an example of how you could do that:

import CallbackExample from "@examples/models/chat/token_usage_tracking_callback.ts";

<CodeBlock language="typescript">{CallbackExample}</CodeBlock>
