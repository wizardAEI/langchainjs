---
sidebar_position: 0
---

# Quick Start

Chat models are a variation on language models.
While chat models use language models under the hood, the interface they use is a bit different.
Rather than using a "text in, text out" API, they use an interface where "chat messages" are the inputs and outputs.

## Setup

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

import UnifiedModelParamsTooltip from "@mdx_components/unified_model_params_tooltip.mdx";

<UnifiedModelParamsTooltip></UnifiedModelParamsTooltip>

<Tabs groupId="preferredModel">
  <TabItem value="openai" label="OpenAI" default>

First we'll need to install the LangChain OpenAI integration package:

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/openai
```

Accessing the API requires an API key, which you can get by creating an account and heading [here](https://platform.openai.com/account/api-keys). Once we have a key we'll want to set it as an environment variable:

```bash
OPENAI_API_KEY="..."
```

If you'd prefer not to set an environment variable you can pass the key in directly via the `apiKey` named parameter when initiating the OpenAI Chat Model class:

```typescript
import { ChatOpenAI } from "@langchain/openai";

const chatModel = new ChatOpenAI({
  apiKey: "...",
});
```

Otherwise you can initialize without any params:

```typescript
import { ChatOpenAI } from "@langchain/openai";

const chatModel = new ChatOpenAI();
```

  </TabItem>
  <TabItem value="local" label="Local (using Ollama)">

[Ollama](https://ollama.ai/) allows you to run open-source large language models, such as Llama 2 and Mistral, locally.

First, follow [these instructions](https://github.com/jmorganca/ollama) to set up and run a local Ollama instance:

- [Download](https://ollama.ai/download)
- Fetch a model via e.g. `ollama pull mistral`

Then, make sure the Ollama server is running. Next, you'll need to install the LangChain community package:

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community
```

And then you can do:

```typescript
import { ChatOllama } from "@langchain/community/chat_models/ollama";

const chatModel = new ChatOllama({
  baseUrl: "http://localhost:11434", // Default value
  model: "mistral",
});
```

  </TabItem>
  <TabItem value="anthropic" label="Anthropic" default>

First we'll need to install the LangChain Anthropic integration package:

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/anthropic
```

Accessing the API requires an API key, which you can get by creating an account [here](https://console.anthropic.com/). Once we have a key we'll want to set it as an environment variable:

```bash
ANTHROPIC_API_KEY="..."
```

If you'd prefer not to set an environment variable you can pass the key in directly via the `apiKey` named parameter when initiating the `ChatAnthropic` class:

```typescript
import { ChatAnthropic } from "@langchain/anthropic";

const chatModel = new ChatAnthropic({
  apiKey: "...",
});
```

Otherwise you can initialize without any params:

```typescript
import { ChatAnthropic } from "@langchain/anthropic";

const chatModel = new ChatAnthropic();
```

  </TabItem>
  <TabItem value="google" label="Google GenAI" default>

First we'll need to install the LangChain Google GenAI integration package:

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/google-genai
```

Accessing the API requires an API key, which you can get by creating an account [here](https://ai.google.dev/tutorials/setup). Once we have a key we'll want to set it as an environment variable:

```bash
GOOGLE_API_KEY="..."
```

If you'd prefer not to set an environment variable you can pass the key in directly via the `apiKey` named parameter when initiating the `ChatGoogleGenerativeAI` class:

```typescript
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";

const chatModel = new ChatGoogleGenerativeAI({
  apiKey: "...",
});
```

Otherwise you can initialize without any params:

```typescript
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";

const chatModel = new ChatGoogleGenerativeAI();
```

  </TabItem>
</Tabs>

## Messages

The chat model interface is based around messages rather than raw text.
The types of messages currently supported in LangChain are `AIMessage`, `HumanMessage`, `SystemMessage`, `FunctionMessage`, and `ChatMessage` -- `ChatMessage` takes in an arbitrary role parameter. Most of the time, you'll just be dealing with `HumanMessage`, `AIMessage`, and `SystemMessage`

## LCEL

Chat models implement the [Runnable interface](/docs/expression_language/interface), the basic building block of the [LangChain Expression Language (LCEL)](/docs/expression_language/). This means they support `invoke`, `stream`, `batch`, and `streamLog` calls.

Chat models accept `BaseMessage[]` as inputs, or objects which can be coerced to messages, including `string` (converted to `HumanMessage`) and `PromptValue`.

```typescript
import { HumanMessage, SystemMessage } from "@langchain/core/messages";

const messages = [
  new SystemMessage("You're a helpful assistant"),
  new HumanMessage("What is the purpose of model regularization?"),
];
```

```typescript
await chatModel.invoke(messages);
```

```
AIMessage { content: 'The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model becomes too complex and starts to fit the noise in the training data, leading to poor generalization on unseen data. Regularization techniques introduce additional constraints or penalties to the model's objective function, discouraging it from becoming overly complex and promoting simpler and more generalizable models. Regularization helps to strike a balance between fitting the training data well and avoiding overfitting, leading to better performance on new, unseen data.' }
```

See the [Runnable interface](/docs/expression_language/interface) for more details on the available methods.

## [LangSmith](https://docs.smith.langchain.com/)

All `ChatModel`s come with built-in LangSmith tracing. Just set the following environment variables:

```bash
export LANGCHAIN_TRACING_V2="true"
export LANGCHAIN_API_KEY=<your-api-key>
```

and any `ChatModel` invocation (whether it's nested in a chain or not) will automatically be traced. A trace will include inputs, outputs, latency, token usage, invocation params, environment params, and more. See an example here: https://smith.langchain.com/public/a54192ae-dd5c-4f7a-88d1-daa1eaba1af7/r.

In LangSmith you can then provide feedback for any trace, compile annotated datasets for evals, debug performance in the playground, and more.

## [Legacy] `generate`

### Batch calls, richer outputs

You can go one step further and generate completions for multiple sets of messages using `generate`. This returns an `LLMResult` with an additional `message` parameter.

```typescript
const response3 = await chatModel.generate([
  [
    new SystemMessage(
      "You are a helpful assistant that translates English to French."
    ),
    new HumanMessage(
      "Translate this sentence from English to French. I love programming."
    ),
  ],
  [
    new SystemMessage(
      "You are a helpful assistant that translates English to French."
    ),
    new HumanMessage(
      "Translate this sentence from English to French. I love artificial intelligence."
    ),
  ],
]);
console.log(response3);
/*
  {
    generations: [
      [
        {
          text: "J'aime programmer.",
          message: AIMessage { text: "J'aime programmer." },
        }
      ],
      [
        {
          text: "J'aime l'intelligence artificielle.",
          message: AIMessage { text: "J'aime l'intelligence artificielle." }
        }
      ]
    ]
  }
*/
```

You can recover things like token usage from this LLMResult:

```typescript
console.log(response3.llmOutput);
/*
  {
    tokenUsage: { completionTokens: 20, promptTokens: 69, totalTokens: 89 }
  }
*/
```
